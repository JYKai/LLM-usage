# 효율적인 배치 전략
언어 모델의 특성상 한 번에 하나씩의 토큰을 생성하고 입력에 따라 몇 개의 토큰을 추가로 생성할지 예측하기 어렵기 때문에 기존의 딥러닝 모델보다 배치 전략을 세우는 데 고려해야 할 사항이 더 많다.

## 일반 배치(정적 배치)
한 번에 N개의 입력을 받아 모두 추론이 끝날 때까지 기다리는 방식
- 배치 중 추론이 이미 끝난 배치로 인해 비효율이 발생한다.

## 동적 배치
비슷한 시간대에 들어오는 요청을 하나의 배치로 묶어 배치 크기를 키우는 전략이다.

동적 배치를 사용하지 않는 경우 요청이 들어올 때마다 추론을 수행하게 되는게 그러면 지연 시간은 짧을 수 있지만 GPU를 효율적으로 사용하지 못한다. 또한, 이후에 들어온 요청을 처리하지 못하고 대기하는 경우도 발생할 수 있다.

동적 배치를 사용하게 되면 약간의 지연 시간이 추가되지만 전체적으로 처리량을 높일 수 있다. 하지만 생성하는 토큰 길이 차이로 인해 처리하는 배치 크기가 점차 줄어 GPU를 비효율적으로 사용하게 되는 문제는 여전히 남게 된다.

## 연속 배치
일반 배치와 달리 한 번에 들어온 배치 데이터의 추론이 모두 끝날 때까지 기다리지 않고 하나의 토큰 생성이 끝날 때마다 생성이 종료된 문장은 제거하고 새로운 문장을 추가한다.

</br></br>

# 효율적인 트랜스포머 연산

## 플래시어텐션
플래시어텐션(FlashAttention)은 트랜스포머가 더 긴 시퀀스를 처리하도록 만들기 위해 개발되었다. 
- 어텐션 연산 과정을 변경해 학습 과정에서 필요한 메모리를 시퀀스 길이에 비례하도록 개선했다.

트랜스포머 연산은 쿼리와 키 벡터를 곱하는 과정에서 많은 메모리를 사용한다.
- 어텐션 연산이 오래 걸리는 이유는 GPU에서 메모리를 읽고 쓰는 데 오랜 시간이 걸리기 때문이다.
- SRAM은 빠르지만 메모리 크기가 작아 대부분의 읽기 쓰기 작업은 고대역폭 메모리(HBM)에서 이뤄진다. 즉, 어텐션 행렬은 크기가 크기 때문에 SRAM에서 처리할 수 없고 저장 공간이 큰 HBM에 쓰고 다시 읽으면서 연산을 수행해야 하기 때문에 데이터 전달 속도가 느린 HBM 특성상 어텐션 행렬을 쓰고 읽는데 오랜 시간이 걸리게 된다.

플래시어텐션에서는 블록 단위로 어텐션 연산을 수행하고 전체 어텐션 행렬을 쓰거나 읽지 않는 방식으로 어텐션 연산의 속도를 높였다. 작은 블록 단위로 어텐션 연산을 수행하기 때문에 SRAM에 데이터를 읽고 쓰면서 더 빠르게 연산을 수행한다.

> **블록 단위의 소프트맥스 연산 과정**  
소프트맥스 연산을 위해서는 분모에서 전체 벡터의 값이 필요한데, 블록 단위로 계산하려면 벡터 일부분의 소프트맥스 연산 결과로 전체 소프트맥스 연산 결과를 만들 수 있어야 한다.  
계산하고자 하는 벡터 X를 n개로 나눠서 계산한다. 분자는 서로 연결해서 하나의 벡터로 만들고 분모는 나눠진 부분에 대해서 추가적으로 서로 더해주면 된다.

플래시어텐션을 사용해서 어텐션 행렬을 저장하지 않고 연산을 할 수 있었지만, 역전파 과정에서 순전파에서 계산한 행렬의 값이 필요하다. 플래시어텐션은 이를 역전파과정에서 다시 순전파를 계산하는 방식을 사용하여 문제를 해결하였다.

## 플래시어텐션 2

**버전 2에서 개선한 부분**  
- 행렬 곱셈이 아닌 연산 줄이기
- 시퀀스 길이 방향의 병렬화 추가

딥러닝 연산에 사용하는 GPU의 경우 행렬 연산에 최적화 되어있다. A100 GPU를 기준으로 FP16 또는 BF16의 행렬 곱셈 연산은 최대 312TFLOPS까지 가능하지만 FP32인 비 행렬 곱셈 연산은 19.5TFLOPS밖에 처리하지 못한다. 
- 즉, 비용 측면에서 보자면 비 행렬 곱셈 연산이 행렬 곱셈 연산에 비해 16배 비싸다고 생각할 수 있다.

기존에는 (배치 크기 x 어텐션 헤드 수)만큼의 스레드 블록으로 병렬처리를 했는데 시퀀스 길이 방향으로 병렬화를 추가했다. GPU의 가장 작은 계산 단위는 스레드이고 GPU에서는 스레드의 모음인 스레드 블록 단위로 병렬 처리를 수행한다.
- GPU에는 여러 개의 스트리밍 멀티프로세서(SM)이 있는데 하나의 스레드 블록은 하나의 SM에 배정돼 처리된다.
- GPU를 효율적으로 활용하기 위해서는 충분한 수의 스레드 블록이 있어야 하는데, 만약 배치 크기가 작거나 어텐션 헤드 수가 작은 경우 GPU의 SM을 충분히 활용하지 못하는 경우가 생긴다.

플래시어텐션 2는 시퀀스 길이 방향으로 여러 개의 묶음으로 나눠 사용하는 스레드 블록 수를 늘려서 기존의 플래시어텐션보다 약 2배의 속도 향상이 있었다.

## 상대적 위치 인코딩
위치 인코딩을 학습할 수 있는 파라미터로 처리하더라도 결국 학습이 끝나는 시점에는 토큰의 절대적인 위치에 따라 정해진 값을 더한다는 점은 동일하다. 이렇듯 위치에 따라 정해진 값을 추가하는 방식을 절대적 위치 인코딩이라고 한다.
- 학습 데이터와 비슷한 입력 데이터에서는 잘 동작하지만, 학습 데이터보다 더 긴 입력이 들어오면 언어 모델의 생성 품질이 빠르게 떨어진단느 한계가 있다.

위의 한계를 극복하기 위해 토큰과 토큰 사이의 상대적인 위치 정보를 추가하는 상대적 위치 인코딩(relative positional encoding) 방식이 활발히 연구됐다.
- RoPE(Rotary Positional Encoding)
    - 각각의 토큰 임베딩을 토큰 위치에 따라 회전시킨다.
    - 토큰 사이의 위치 정보가 두 엠비딩 사이의 각도를 통해 모델에 반영된다.
- ALiBl(Attention with Linear Biases)
    - 왼쪽의 쿼리와 키 벡터를 곱한 어텐션 행렬에 오른쪽에서 왼쪽으로 갈수록 더 작은 값을 더하는 방식을 사용한다.

# 효율적인 추론 전략

## 커널 퓨전(Kernel Fusion)
GPU에서 연산은 커널 단위로 이뤄진다. 커널이 수행되는 경우 데이터를 읽고 쓰는 것과 같은 오버헤드가 발생하게 되는데 N개의 커널이 수행되는 경우 오버헤드도 그만큼 많이 발생하게 된다. 따라서 반복적으로 수행하는 연산에 대해서는 각 커널 단위로 분리해서 수행하는 것보다 연산을 하나로 묶어 오버헤드를 줄이는 방법을 커널 퓨전이라고 한다.

## 페이지어텐션
기존의 KV 캐시는 앞으로 사용할 수도 있는 메모리를 미리 잡아두면서 GPU 메모리를 많이 낭비한다는 문제가 있었다. 페이지어텐션에서는 운영체제의 가상 메모리 개념을 빌려와 중간에서 논리적 메모리와 물리적 메모리를 연결하는 블록 테이블을 관리해서 실제로는 물리적으로 연속된 메모리를 사용하지 않으면서도 논리적 메모리에서는 서로 연속적이도록 만들었다.
- 블록 단위로 메모리를 배정하기 때문에 한 번에 큰 메모리를 예약하지 않고 최대 블록 크기만큼의 메모리만 배정할 수 있다. 즉, 블록 크기가 4이면 배정후에 생성이 종료돼 사용하지 않게 되더라도 최대 (블록 크기 - 1)개의 메모리만 낭비하기 때문에 낭비되는 메모리양을 훨씬 줄일 수 있다.
- 다양한 디코딩 방식에서 메모리를 절약할 수 있다.
    - 물리적 블록을 공유하고 있는 논리적 블록 수를 의미하는 참조 카운트라는 개념을 활용하여 참조 카운트가 1보다 클 경우 새로운 토큰을 바로 이어서 저장하지 않고 새로운 물리적 블록에 기존 토큰을 복사해서 새롭게 분리한다.
    - 동일한 토큰은 최대한 메모리를 공유하고 새롭게 생성하는 토큰만 서로 분리해 저장하면서 GPU 메모리를 효율적으로 활용할 수 있다.

## 추측 디코딩(speculative decoding)
쉬운 단어는 더 작고 효율적인 모델이 예측하고 어려운 단어는 더 크고 성능이 좋은 모델이 예측하는 방식
- 드래프트 모델(draft model)
    - 작은 드래프트 모델은 큰 모델에 비해 빠르게 추론이 가능하므로 K개의 토큰을 먼저 생성한다.
- 타깃 모델(target model)
    - 타깃 모델이 드래프트 모델이 생성한 토큰이 타깃 모델이 추론했다면 생성했을 결과와 동일한지 계산해 동일하다고 판단되면 승인하고 그렇지 않다면 거절한다.
    - 드래프트 모델은 빠르지만 생성 정확도는 떨어지기 때문에 드래프트 모델로 어림짐작하고 타깃 모델로 그 짐작을 확인하면서 추론을 진행한다는 의미

추측 디코딩은 원본 모델에 비해 훨씬 작은 드래프트 모델의 추가만으로 원본 모델의 성능을 그대로 유지하면서 속도를 2배 이상 높일 수 있다는 점 때문에 다양한 서빙 프레임워크에서 채택하고 있다.
- 하지만, 추가적인 모델이 필요하기 때문에 시스템 복잡도가 올라간다는 단점이 있다.

# LLM 서빙 프레임워크

## 오프라인 서빙
오프라인 서빙이란 정해진 입력 데이터에 대해 배치 추론을 수행하는 것을 말한다. 모델에 입력할 데이터가 이미 정해져 있기 때문에 배치 처리를 통해 처리량을 높일 수 있다.

## 온라인 서빙
온라인 서빙은 사용자의 요청이 올 때 모델 추론을 수행한다는 점에서 오프라인 서빙과 다르다.
- vLLM 프레임워크는 온라인 서빙에 사용할 수 있는 API 서버를 제공한다.
- vLLM의 API 서버에는 OpenAI의 API와 동일한 형태로 구현되어 있어 OpenAI 클라이언트를 사용해서도 요청을 전달할 수 있다.