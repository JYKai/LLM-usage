{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b95445f4-1f6e-412c-af8b-bbde731c75ef",
   "metadata": {},
   "source": [
    "# LLM 서빙 프레임워크"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ed3715-52ef-4708-af12-e1108603aada",
   "metadata": {},
   "source": [
    "## 오프라인 서빙"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f47ac1-0653-48d4-a4bf-93c18a7dae48",
   "metadata": {},
   "source": [
    "**데이터셋 준비**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c57c642-b6c5-4474-9f47-234d32e62103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ede5e14-e063-440b-9d40-d3bb3d2964a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(ddl, question, query=''):\n",
    "    prompt = f\"\"\"당신은 SQL을 생성하는 SQL 봇입니다. DDL의 테이블을 활용한 Question을 해결할 수 있는 SQL 쿼리를 생성하세요.\n",
    "\n",
    "### DDL:\n",
    "{ddl}\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### SQL:\n",
    "{query}\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25cf7fc8-3756-4c84-aa59-6ffb322160ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"shangrilar/ko_text2sql\", \"origin\")['test']\n",
    "dataset = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16fe80d0-8d6b-4c9b-822b-6ed248113efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in dataset.iterrows():\n",
    "    prompt = make_prompt(row['context'], row['question'])\n",
    "    dataset.loc[idx, 'prompt'] = prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5b8238-0104-42b5-92a1-0401ce1b3c86",
   "metadata": {},
   "source": [
    "**모델과 토크나이저를 불러와 추론 파이프라인 준비**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a743c621-ddb4-4071-9d4a-3f852eceedc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-28 02:18:05.651003: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-28 02:18:05.657983: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-28 02:18:05.666003: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-28 02:18:05.668419: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-28 02:18:05.674948: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-28 02:18:06.084033: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dddef7bf-e352-40bb-99d5-f5ec985fabe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df0197d44314b34a5778a3f41c0d80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a83d8d602b4ce487c8e691951cc764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b0fc0e88284f78a237489e9a7ef15a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f41c2cb253f4ac9b5af456980b88d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72115d1f52984352863dd6fe7ba27ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2dd5d5a6a1a4840b6a97731a4d95a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe054ce72cbe4c1684ec44bb60c55826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c55f59caab421e9ac9e28c9d10f96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "701a7c497d344280af7db0e3d66de90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/9.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ec92895c2a4dc8b451ac6e8b8fccce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/4.28M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120007fc78df4f568966387d418279ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/467 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"shangrilar/yi-ko-6b-text2sql\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "hf_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f71260-f322-4129-80c4-261396a70449",
   "metadata": {},
   "source": [
    "**배치 크기에 따른 추론 시간 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af48e1ca-2337-4d09-a861-280fed358d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b48c09b4-ec9e-4d14-80b6-0b3327147c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 63.74052119255066\n",
      "2: 68.76405763626099\n",
      "4: 46.17130756378174\n",
      "8: 32.09809470176697\n",
      "16: 23.121740341186523\n",
      "32: 20.11572813987732\n"
     ]
    }
   ],
   "source": [
    "for batch_size in [1, 2, 4, 8, 16, 32]:\n",
    "    start_time = time.time()\n",
    "    hf_pipeline(dataset['prompt'].tolist(), max_new_tokens=128, batch_size=batch_size)\n",
    "    print(f\"{batch_size}: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d10f780e-876f-4668-b97b-394b42d4f5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def cleanup():\n",
    "    if 'model' in globals():\n",
    "        del globals()['model']\n",
    "    if 'dataset' in globals():\n",
    "        del globals()['dataset']\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f52143d7-472b-4192-a5b8-0f34456613aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ab39c6d-4a5f-49c3-a21d-83da0ce1e46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated Memory: 20889.91 MB\n",
      "Cached Memory: 20964.00 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Allocated Memory: {torch.cuda.memory_allocated() / 1024 ** 2:.2f} MB\")\n",
    "print(f\"Cached Memory: {torch.cuda.memory_reserved() / 1024 ** 2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6838235b-d75b-4e2c-88d6-6973e75af9d6",
   "metadata": {},
   "source": [
    "**vLLM 모델 불러오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0cecfa0-058c-49c9-ad44-31491f2e1b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94bb3e23-5e7e-4cf3-87a6-bf9c26c81563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-28 02:18:11 llm_engine.py:98] Initializing an LLM engine (v0.4.1) with config: model='shangrilar/yi-ko-6b-text2sql', speculative_config=None, tokenizer='shangrilar/yi-ko-6b-text2sql', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-28 02:18:12 utils.py:608] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 10-28 02:18:12 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.\n",
      "INFO 10-28 02:18:12 selector.py:33] Using XFormers backend.\n",
      "INFO 10-28 02:18:18 weight_utils.py:193] Using model weights format ['*.safetensors']\n",
      "INFO 10-28 02:18:21 model_runner.py:173] Loading model weights took 11.5127 GB\n",
      "INFO 10-28 02:18:21 gpu_executor.py:119] # GPU blocks: 9049, # CPU blocks: 4096\n",
      "INFO 10-28 02:18:22 model_runner.py:976] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-28 02:18:22 model_runner.py:980] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-28 02:18:27 model_runner.py:1057] Graph capturing finished in 5 secs.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"shangrilar/yi-ko-6b-text2sql\"\n",
    "llm = LLM(model=model_id, dtype=torch.float16, max_model_len=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c649769-5178-473a-9b01-5f77be205b3f",
   "metadata": {},
   "source": [
    "**vLLM을 활용한 오프라인 추론 시간 측정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5beb059-01ff-4638-855c-7a19e03558b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m llm\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mscheduler_config\u001b[38;5;241m.\u001b[39mmax_num_seqs \u001b[38;5;241m=\u001b[39m max_num_seqs\n\u001b[1;32m      4\u001b[0m sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m output \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mgenerate(\u001b[43mdataset\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list(), sampling_params)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_num_seqs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "for max_num_seqs in [1, 2, 4, 8, 16, 32]:\n",
    "    start_time = time.time()\n",
    "    llm.llm_engine.scheduler_config.max_num_seqs = max_num_seqs\n",
    "    sampling_params = SamplingParams(temperature=1, top_p=1, max_tokens=128)\n",
    "    output = llm.generate(dataset['prompt'].to_list(), sampling_params)\n",
    "    print(f\"{max_num_seqs}: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a5787e-f21f-4ca0-9bd9-038d6d82be4d",
   "metadata": {},
   "source": [
    "## 온라인 서빙"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43092c26-5434-49cb-844f-64aeff90c021",
   "metadata": {},
   "source": [
    "**온라인 서빙을 위한 vLLM API 서버 실행**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "135ac440-9ac5-467c-a3d8-a78b81898049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-28 02:21:07 api_server.py:151] vLLM API server version 0.4.1\n",
      "INFO 10-28 02:21:07 api_server.py:152] args: Namespace(host='127.0.0.1', port=8890, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='shangrilar/yi-ko-6b-text2sql', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=1024, guided_decoding_backend='outlines', worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, speculative_model=None, num_speculative_tokens=None, speculative_max_model_len=None, model_loader_extra_config=None, engine_use_ray=False, disable_log_requests=False, max_log_len=None)\n",
      "INFO 10-28 02:21:07 llm_engine.py:98] Initializing an LLM engine (v0.4.1) with config: model='shangrilar/yi-ko-6b-text2sql', speculative_config=None, tokenizer='shangrilar/yi-ko-6b-text2sql', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO 10-28 02:21:07 utils.py:608] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 10-28 02:21:07 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.\n",
      "INFO 10-28 02:21:07 selector.py:33] Using XFormers backend.\n",
      "INFO 10-28 02:21:08 weight_utils.py:193] Using model weights format ['*.safetensors']\n",
      "INFO 10-28 02:21:10 model_runner.py:173] Loading model weights took 11.5127 GB\n",
      "INFO 10-28 02:21:11 gpu_executor.py:119] # GPU blocks: 9049, # CPU blocks: 4096\n",
      "INFO 10-28 02:21:12 model_runner.py:976] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-28 02:21:12 model_runner.py:980] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-28 02:21:14 model_runner.py:1057] Graph capturing finished in 3 secs.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING 10-28 02:21:15 serving_chat.py:347] No chat template provided. Chat API will not work.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m1528\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:8890\u001b[0m (Press CTRL+C to quit)\n",
      "INFO 10-28 02:21:25 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 10-28 02:21:35 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 10-28 02:21:45 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 10-28 02:21:55 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 10-28 02:22:05 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "^C\n",
      "\u001b[32mINFO\u001b[0m:     Shutting down\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
      "\u001b[32mINFO\u001b[0m:     Application shutdown complete.\n",
      "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m1528\u001b[0m]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 169, in <module>\n",
      "    uvicorn.run(app,\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/main.py\", line 575, in run\n",
      "    server.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/server.py\", line 65, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "  File \"/usr/lib/python3.10/asyncio/runners.py\", line 44, in run\n",
      "    return loop.run_until_complete(main)\n",
      "  File \"uvloop/loop.pyx\", line 1512, in uvloop.loop.Loop.run_until_complete\n",
      "  File \"uvloop/loop.pyx\", line 1505, in uvloop.loop.Loop.run_until_complete\n",
      "  File \"uvloop/loop.pyx\", line 1379, in uvloop.loop.Loop.run_forever\n",
      "  File \"uvloop/loop.pyx\", line 557, in uvloop.loop.Loop._run\n",
      "  File \"uvloop/loop.pyx\", line 476, in uvloop.loop.Loop._on_idle\n",
      "  File \"uvloop/cbhandles.pyx\", line 83, in uvloop.loop.Handle._run\n",
      "  File \"uvloop/cbhandles.pyx\", line 63, in uvloop.loop.Handle._run\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/server.py\", line 68, in serve\n",
      "    with self.capture_signals():\n",
      "  File \"/usr/lib/python3.10/contextlib.py\", line 142, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/server.py\", line 328, in capture_signals\n",
      "    signal.raise_signal(captured_signal)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python3 -m vllm.entrypoints.openai.api_server \\\n",
    "--model shangrilar/yi-ko-6b-text2sql --host 127.0.0.1 --port 8890 --max-model-len 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c811edd5-e1e0-434b-b9af-782af01b0b65",
   "metadata": {},
   "source": [
    "**백그라운드에서 vLLM API 서버 실행하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03e14a0a-d4e0-413f-9654-b2223c86cbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-28 02:24:19 api_server.py:151] vLLM API server version 0.4.1\n",
      "INFO 10-28 02:24:19 api_server.py:152] args: Namespace(host='127.0.0.1', port=8890, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='shangrilar/yi-ko-6b-text2sql', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=1024, guided_decoding_backend='outlines', worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, speculative_model=None, num_speculative_tokens=None, speculative_max_model_len=None, model_loader_extra_config=None, engine_use_ray=False, disable_log_requests=False, max_log_len=None)\n",
      "INFO 10-28 02:24:19 llm_engine.py:98] Initializing an LLM engine (v0.4.1) with config: model='shangrilar/yi-ko-6b-text2sql', speculative_config=None, tokenizer='shangrilar/yi-ko-6b-text2sql', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-28 02:24:19 utils.py:608] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 10-28 02:24:19 selector.py:77] Cannot use FlashAttention backend because the flash_attn package is not found. Please install it for better performance.\n",
      "INFO 10-28 02:24:19 selector.py:33] Using XFormers backend.\n",
      "INFO 10-28 02:24:20 weight_utils.py:193] Using model weights format ['*.safetensors']\n",
      "INFO 10-28 02:24:22 model_runner.py:173] Loading model weights took 11.5127 GB\n",
      "INFO 10-28 02:24:23 gpu_executor.py:119] # GPU blocks: 9049, # CPU blocks: 4096\n",
      "INFO 10-28 02:24:23 model_runner.py:976] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-28 02:24:23 model_runner.py:980] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-28 02:24:26 model_runner.py:1057] Graph capturing finished in 3 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-28 02:24:27 serving_chat.py:347] No chat template provided. Chat API will not work.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:     Started server process [1950]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8890 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-28 02:24:37 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 10-28 02:24:47 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "Process is interrupted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [1950]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "nohup python3 -m vllm.entrypoints.openai.api_server \\\n",
    "--model shangrilar/yi-ko-6b-text2sql --host 127.0.0.1 --port 8890 --max-model-len 1024 &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d9f2b-4b3a-49b4-91b8-c5472abd1c45",
   "metadata": {},
   "source": [
    "**API 서버 실행 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "044d6855-7c6b-48e1-847a-3f11dd8087cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: curl: not found\n"
     ]
    }
   ],
   "source": [
    "!curl http://localhost:8890/v1/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0950ea08-127d-4407-bb6c-caa04d476a2f",
   "metadata": {},
   "source": [
    "**API 요청**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f634a7be-530e-459b-bd38-8cd539cfd62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79d0394-bc8e-49d3-aad1-b8c048e57d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = json.dumps(\n",
    "    {\"model\": \"shangrilar/yi-ko-6b-text2sql\",\n",
    "     \"prompt\": dataset.loc[0, \"prompt\"],\n",
    "     \"max_tokens\": 128,\n",
    "     \"temperature\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b40521-8533-48fc-8b81-5e0054f74228",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://localhost:8888/v1/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{json_data}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c052ddf9-dac5-4780-8cd4-abc157e011ec",
   "metadata": {},
   "source": [
    "**OpenAI 클라이언트를 사용한 API 요청**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a23bea-1c0b-44b4-bd37-2489f9ad4058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "012386ab-fbff-4a05-a95c-595c32c3a1a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 8) (2570726381.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(\"생성 결과:\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 8)\n"
     ]
    }
   ],
   "source": [
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8888/v1\"\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "completion = client.completions.create(model=\"shangrilar/yi-ko-6b-text2sql\", prompt=dataset.loc[0, 'prompt'], max_tokens=128)\n",
    "print(\"생성 결과:\", completion.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd64d8fd-b7d8-4114-9266-d1b754540c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
