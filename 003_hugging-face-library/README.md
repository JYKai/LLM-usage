# 허깅페이스 트랜스포머란?
허깅페이스 트랜스포머는 다양한 트랜스포머 모델을 통일된 인터페이스로 사용할 수 있도록 지원하는 오픈소스 라이브러리이다.

크게, 토크나이저를 활용할 때 사용하는 transformers 라이브러리와 데이터셋을 공개하고 쉽게 가져다 쓸 수 있도록 지원하는 datasets 라이브러리를 제공한다.

</br>

# 허깅페이스 허브 탐색하기
허깅페이스의 허브는 다양한 사전 학습 모델과 데이터셋을 탐색하고 쉽게 불러와 사용할 수 있도록 제공하는 온라인 플랫폼이다.
- 모델 데모를 제공하고 다른 사람의 모델을 사용해 볼 수 있는 스페이스도 있다.

## 모델 허브
모델 허브에는 어떤 작업에 사용하는지, 어떤 언어로 학습된 모델인지 등 다양한 기준으로 모델이 분류되어 있다.

## 데이터셋 허브
모델 허브와 달리 분류 기준에 데이터셋 크기, 데이터 유형 등이 추가로 있고 선택한 기준에 맞는 데이터셋을 보여준다는 점이 다르다.

## 모델 데모를 공개하고 사용할 수 있는 스페이스
사용자가 자신의 모델 데모를 간편하게 공개할 수 있는 기능이다.

</br>

# 허깅페이스 라이브러리 사용법 익히기

## 모델 활용하기
허깅페이스에는 모델을 바디와 헤드로 구분한다.
- 바디를 사용하면서 다른 작업에 사용할 수 있도록 만들기 위함이다.
- 모델의 바디만 불러올 수 있고, 헤드와 함께 불러올 수도 있다.

```python
from transformers import AutoModel

model_id = 'klue/roberta-base'
model = AutoModel.from_pretrained(model_id)
```
- `AutoModel` : 모델의 바디를 불러오는 클래스
    - `from_pretrained()` 메서드에서 인자로 받는 model_id에 맞춰 적절한 클래스를 가져온다.
    - 모델 ID가 허깅페이스 모델 허브의 저장소 경로인 경우 모델 허브에서 모델을 다운로드하고, 로컬 경로인 경우 지정한 로컬 경로에서 모델을 불러온다.

```python
from transformers import AutoModelForSequenceClassification

model_id = 'SamLowe/roberta-base-go_emotions'
classification_model = AutoModelForSequenceClassification.from_pretrained(model_id)
```
- `AutoModelForSequenceClassification` : 분류를 위한 모델을 불러오는 클래스

## 토크나이저 활용하기
토크나이저는 텍스트를 토큰 단위로 나누고 각 토큰을 대응하는 토큰 아이디로 변환한다.
- 토크나이저도 학습 데이터를 통해 어휘 사저을 구축하기 때문에 일반적으로 모델과 함께 저장한다.
- 허깅페이스 허브에서 모델과 토크나이저를 불러오는 경우 동일한 모델 아이디로 맞춰야 한다.

```python
from transformers import AutoTokenizer

model_id = 'klue/roberta-base'
tokenizer = AutoTokenizer.from_pretrained(model_id)
```
- 토크나이저에 텍스트를 입력하면 아래의 결과를 반환한다.
    - input_ids : 아이디의 리스트
        - 토큰화했을 때 각 토큰이 토크나이저 사전의 몇 번째 항목인지를 나타낸다.
    - attention_mask : 토큰이 실제 텍스트인지 아니면 길이를 맞추기 위해 추가한 패딩인지 알려주는 정보
        - 1일 경우 패딩이 아닌 실제 토큰임을 의미
    - token_type_ids : 토큰이 속한 문장의 아이디를 알려주는 정보
        - 0일 경우 일반적으로 첫 번째 문장임을 의미한다.
- `decode` 메서드를 사용하면 토큰 아이디를 다시 텍스트로 돌릴 수 있다.
    - [CLS], [SEP] 같은 특수 토큰을 제외하고 싶다면 `skip_special_tokens` 인자를 `True`로 설정한다.

- 한 번에 여러 문장을 넣을 수 있으며, 예를 들어, 2개의 문장이 하나의 데이터라는 것을 표시하기 위해 한 번 더 리스트로 감싸줘서 입력해준다.
- 토크나이저의 `batch_decode()` 메서드를 사용하면 input_ids 부분의 토큰 아이디를 문자열로 복원할 수 있다.
    - 기본적으로 토큰화를 하면 [CLS] 토큰으로 문장을 시작하고, [SEP]으로 문장을 끝낸다.
    - 2개의 문장을 한번에 토큰화하면 [SEP]으로 두 문장을 구분한다.
    - 특수 토큰은 모델의 아키텍처에 따라 달라질 수 있으니 사용하려는 토크나이저가 어떤 특수 토큰을 사용하는지 확인해 볼 필요가 있다.

## 데이터셋 활용하기
`datasets` 라이브러리를 활용하면 허깅페이스 허브에서 살펴봤던 데이터셋을 코드로 불러올 수 있다.

</br>

# 모델 학습시키기
허깅페이스 트랜스포머에서는 간편하게 모델 학습을 수행할 수 있도록 학습 과정을 추상화한 트레이너 API를 제공한다.
- 학습이 간단해지지만, 내부에서 어떤 과정을 거치는지 알기 어렵다는 단점이 있다.

## 트레이너 API를 사용해 학습하기
허깅페이스는 학습에 필요한 다양한 기능(데이터로더 준비, 로깅, 평가, 저장 등)을 학습 인자(TrainingArguments)만으로 쉽게 활용할 수 있는 트레이너 API를 제공한다.

## 트레이너 API를 사용하지 않고 학습하기

</br>

# 모델 추론하기

## 파이프라인을 활용한 추론
파이프라인은 크게 작업 종류, 모델, 설정을 입력으로 받는다.
- 작업 종류는 텍스트 분류, 토큰 분류 등 작업에 맞춰 설정하고 모델에 저장소 아이디를 설정하면 된다.

## 직접 추론하기
직접 모델과 토크나이저를 불러와 pipeline과 유사하게 추론을 구현