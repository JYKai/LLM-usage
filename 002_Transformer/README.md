# 트랜스포머 아키텍처란?
트랜스포머는 셀프 어텐션(self-attention)이라는 개념을 도입하였으며 이는 입력된 문장 내의 각 단어가 서로 어떤 관련이 있는지 계산해서 각 단어의 표현(representation)을 조정하는 역할을 한다.
- 확장성, 효율성, 다 긴 입력 처리에 용이해졌다.

## 트랜스 포머의 구조
1. 인코더
- 언어를 이해하는 역할
2. 디코더
- 언어를 생성하는 역할

# 텍스트를 임베딩으로 변환하기
텍스트를 모델에 입력할 수 있는 숫자형 데이터인 임베딩으로 변환하기 위해서는 크게 세 가지 과정을 거쳐야 한다.  

1. 토큰화(tokenization)  
텍스트를 적절한 단위로 잘라 숫자형 ID를 부여
    - 토큰화를 할 때는 어떤 토큰이 어떤 숫자 아이디로 연결됐는지 기록해 둔 사전을 만들어야 한다.
    - 단어를 단위로 토큰화하는 경우 어떤 단어를 몇 번으로 변환했는지 모두 저장한다.
        - 이전에 본 적이 없는 새로운 단어는 사전에 없기 때문에 처리하지 못하는 OOV(Out Of Vocabulary) 문제가 자주 발생한다.
    - 작은 단위와 큰 단위 모두 각각의 장단점이 뚜렷하다.
    - 최근에는 데이터에 등장하는 빈도에 따라 토큰화 단위를 결정하는 서브워드 토큰화 방식을 사용한다.
        - 자주 나오는 단어는 단어 단위 그대로 유지
        - 가끔 나오는 단어는 더 작은 단위로 나눠 텍스트의 의미를 최대한 유지
        - 한글의 경우 보통 음절과 단어 사이에서 토큰화된다.

2. 토큰 임베딩으로 변환  
토큰 아이디 -> 토큰 임베딩 층 -> 토큰 임베딩
    - 토큰에서 부여한 토큰 아이디는 하나의 숫자일 뿐이므로 토큰의 의미를 담을 수 없다. 의미를 담기 위해서는 최소 2개 이상의 숫자 집합인 벡터여야 한다.
    - 데이터를 의미를 담아 숫자 집합으로 변환하는 것을 임베딩이라고 한다.
    - 머신러닝과 다르게 딥러닝에서는 모델이 특정 작업을 잘 수행하도록 학습하는 과정에서 데이터의 의미를 잘 담은 임베딩을 만드는 방법도 함께 학습한다.

3. 위치 인코딩  
토큰 임베딩 + 위치 임베딩 -> 입력 임베딩
    - RNN과 트랜스포머의 가장 크 차이점은 입력을 순차적으로 처리하는지 여부이다.
    - 트랜스포머는 순차적인 처리 방식을 버리고 모든 입력을 동시에 처리하므로 순서 정보가 사라지게 된다.
    - 위치 인코딩도 위치에 따른 임베딩층을 추가해 학습 데이터를 통해 학습하는 방식을 많이 활용하고 있다.
    - 절대적 위치 인코딩 : 입력 토큰의 위치에 따라 고정된 임베딩을 더해주는 방법
        - 구현이 쉽지만, 토큰과 토큰 사이의 상대적인 위치 정보는 활용하지 못하고, 학습 데이터에서 보기 어려웠던 긴 텍스트를 추론하는 경우에는 성능이 떨어진다는 문제가 발생한다.
    - 상대적 위치 인코딩 방식도 많이 활용된다.

# 어텐션 이해하기

## 사람이 글을 읽는 방법과 어텐션
어텐션이란 사람이 단어 사이의 관게를 고민하는 과정을 딥러닝 모델이 수행할 수 있도록 모방한 연산이다.  
- 단어와 단어 사이의 관계를 계산해서 그 값에 따라 관련이 깊은 단어와 그렇지 않은 단어를 구분
- 관련이 깊은 단어는 더 많이, 관련이 적은 단어는 더 적게 맥락을 반영

## 쿼리, 키, 값 이해하기
검색창에서 검색을 할 때를 비유로 들면
- 쿼리 : 우리가 입력하는 검색어
- 키 : 쿼리와 관련이 있는지 계산하기 위해 문서가 가진 특징
- 값 : 검색 엔진이 쿼리와 관련이 깊은 키를 가진 문서를 찾아 관련도순으로 정렬해서 문서를 제공할 때 문서. 즉, 우리가 검색하면서 원하는 것은 '값'이다.

## 멀티 헤드 어텐션
직관적으로 이해하자면, 토큰 사이의 관계를 한 가지 측면에서 이해하는 것보다 여러 측면을 동시에 고려할 때 언어나 문장에 대한 이해도가 높아질 것이다.


# 정규화와 피드 포워드 층

## 층 정규화 이해하기
딥러닝 모델에 데이터를 입력할 때, 입력 데이터의 분포가 서로 다르면 모델의 학습이 잘되지 않기 때문에, 데이터를 정규화하여 입력하는 것이 중요하다.
- 데이터를 정규화하여 모든 입력 변수가 비슷한 범위와 분포를 갖도록 조정하는 것
- 딥러닝 분야에서는 층과 층 사이에 정규화를 추가해 학습을 안정적으로 만드는 기법을 사용해왔다.
    - 정규화는 여러 데이터의 평균과 표준편차를 구해 사용한다.

딥러닝에서는 평균과 표준편차를 구할 데이터를 어떻게 묶는지에 따라 크게 배치 정규화와 층 정규화로 구분한다.
- 일반적으로 이미지 처리에서는 배치 정규화를, 자연어 처리에서는 층 정규화를 사용한다.

자연어 처리에서는 입력으로 들어가는 문장의 길이가 다양한데, 배치 정규화를 사용할 경우 정규화에 포함되는 데이터의 수가 제각각이라 정규화 효과를 보장하기 어렵다.
- 층 정규화는 각 토큰 임베딩의 평균과 표준편차를 구해 정규화를 수행한다.
- 문장별로 실제 데이터의 수가 다르더라도 각각의 토큰 임베딩별로 정규화를 수행하기 때문에 정규화 효과에 차이가 없다.

## 피드 포워드 층
피드 포워드 층은 데이터의 특징을 학습하는 완전 연결 층을 말한다.
- 멀티 헤드 어텐션이 단어 사이의 관계를 파악하는 역할이라면 피드 포워드 층은 입력 텍스트 전체를 이해하는 역할을 담당한다.


# 인코더
멀티 헤드 어텐션, 층 정규화, 피드 포워드 층이 반복되는 형태


# 디코더
디코더 블록에서는 마스크 멀티 헤드 어텐션을 사용한다.
- 디코더는 생성을 담당하는 부분으로 앞에서 생성한 토큰을 기반으로 다음 토큰을 생성한다.
- 실제 텍스트를 생성할 때 디코더는 이전까지 생성한 텍스트만 확인할 수 있다.
    - 학습할 때는 인코더, 디코더 모두 완성된 텍스트를 입력으로 받기 때문에 어텐션을 그대로 활용하면 미래 시점에 작성해야 하는 텍스트를 미리 확인하게 되는 문제가 발생한다.  

디코더에는 크로스 어텐션이 있다.
- 인코더의 결과를 디코더가 활용하는 것


# 주요 사전 학습 메커니즘

## 인과적 언어 모델링
문장의 시작부터 끝까지 순차적으로 단어를 에측하는 방식
- 사람의 경우 어릴 때부터 많은 글을 보면서 'A라는 단어 다음에는 B라는 단어가 자주온다'는 직관을 배운다.

## 마스크 언어 모델링
직관적으로 이해하자면 시끄러운 공간에서 대화를 나누는 경우를 상상하면 된다.
- 마스크 언어 모델링은 입력 단어의 일부를 마스크 처리하고 그 단어를 맞추는 작업으로 모델을 학습시킨다.