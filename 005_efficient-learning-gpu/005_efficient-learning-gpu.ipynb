{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c653847a-71ff-45dd-a947-4d4ba195feae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.40.1 datasets==2.19.0 accelerate==0.30.0 peft==0.10.0 bitsandbytes==0.43.1 -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfa477d-03ea-4edd-be80-dc9e6882f3f9",
   "metadata": {},
   "source": [
    "# GPU에 올라가는 데이터 살펴보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723aa68f-e595-4829-b354-4198cf63de8d",
   "metadata": {},
   "source": [
    "## GPU 메모리 분해하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b751a247-5efc-431c-a168-4f20cad433f8",
   "metadata": {},
   "source": [
    "**메모리 사용량 측정을 위한 함수 구현**  \n",
    "- `torch.cuda.memory_allocated()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6741cbb3-ed91-4382-8bcb-53cdb7eee8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fee2948-5904-44cd-b283-e9ce978f2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_utilization():\n",
    "    if torch.cuda.is_available():\n",
    "        used_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "        print(f\"GPU memory usage: {used_memory:.3f} GB\")\n",
    "    else:\n",
    "        print(\"Change Runtime type to GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d26cbb7-0540-4ec8-af74-9d2305b19737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory usage: 0.000 GB\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f278a0-c862-400b-a528-2feb77a62481",
   "metadata": {},
   "source": [
    "**모델을 불러오고 GPU 메모리와 데이터 타입 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86d85b29-4d01-4e7d-9011-36573131a7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_model_and_tokenizer(model_id, peft=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if peft is None:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map={\"\": 0})\n",
    "    print_gpu_utilization()\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d2981ae-edda-4e55-b8d4-7add862f16de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5a0317dc484082a0edbd44a0ddb50d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory usage: 2.599 GB\n",
      "Model data type:  torch.float16\n"
     ]
    }
   ],
   "source": [
    "model_id = \"EleutherAI/polyglot-ko-1.3b\"\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(model_id)\n",
    "print(\"Model data type: \", model.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96380302-e87b-4138-9afa-9917519e4c8d",
   "metadata": {},
   "source": [
    "**그레이디언트와 옵티마이저 상태의 메모리 사용량을 계산하는 함수**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2377ca81-c711-40d3-b593-41041a2518e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Check gradient memory usage\n",
    "def estimate_memory_of_gradients(model):\n",
    "    total_memory = 0\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            total_memory += param.grad.nelement() * param.grad.element_size()\n",
    "    return total_memory\n",
    "\n",
    "# Check optimizer state memory usage\n",
    "def estimate_memory_of_optimizer(optimizer):\n",
    "    total_memory = 0\n",
    "    for state in optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if torch.is_tensor(v):\n",
    "                total_memory += v.nelement() * v.element_size()\n",
    "    return total_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953228b1-2a7b-4d2b-864a-07e6fab6d888",
   "metadata": {},
   "source": [
    "**모델의 학습 과정에서 메모리 사용량을 확인하는 train_model 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "876e3c82-054c-41db-93ff-1e2630c3965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, training_args):\n",
    "    if training_args.gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    train_dataloader = DataLoader(dataset, batch_size=training_args.per_device_train_batch_size)\n",
    "    optimizer = AdamW(model.parameters())\n",
    "    gpu_utilization_printed = False\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader, start=1):\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss = loss / training_args.gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        if step % training_args.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            gradient_memory = estimate_memory_of_gradients(model)\n",
    "            optimizer_memory = estimate_memory_of_optimizer(optimizer)\n",
    "            if not gpu_utilization_printed:\n",
    "                print_gpu_utilization()\n",
    "                gpu_utilization_printed = True\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "    print(f\"Optimizer state memory usage: {optimizer_memory / (1024 ** 3):.3f} GB\")\n",
    "    print(f\"Gradient memory usage: {gradient_memory / (1024 ** 3):.3f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0380a713-4b48-4107-b2c8-1aab74a2227e",
   "metadata": {},
   "source": [
    "**랜덤 데이터셋을 생성하는 make_dummy_dataset 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "543026b4-feb6-4adc-8375-9ad2f2c5e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c7e9523-e128-4057-af1d-6bf8fe342cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dummy_dataset():\n",
    "    seq_len, dataset_size = 256, 64\n",
    "    dummy_data = {\n",
    "        \"input_ids\": np.random.randint(100, 30000, (dataset_size, seq_len)),\n",
    "        \"labels\": np.random.randint(100, 30000, (dataset_size, seq_len)),\n",
    "    }\n",
    "    dataset = Dataset.from_dict(dummy_data)\n",
    "    dataset.set_format(\"pt\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6620caf2-a0ac-4135-bd12-02b39536084f",
   "metadata": {},
   "source": [
    "**더 이상 사용하지 않는 GPU 메모리를 반환하는 cleanup 함수**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa108f2b-ba61-4276-af66-fd79a846ba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b2232ee-6efa-44c8-aa30-d1297056fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    if 'model' in globals():\n",
    "        del globals()['model']\n",
    "    if 'dataset' in globals():\n",
    "        del globals()['dataset']\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902702d1-952b-44b7-b8a4-dad49b0dfb52",
   "metadata": {},
   "source": [
    "**GPU 사용량을 확인하는 gpu_memory_experiment 함수 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7815bd86-bbcf-403a-ba03-888988e5c959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "def gpu_memory_experiment(batch_size,\n",
    "                          gradient_accumulation_steps=1,\n",
    "                          gradient_checkpointing=False,\n",
    "                          model_id=\"EleutherAI/polyglot-ko-1.3b\",\n",
    "                          peft=None):\n",
    "    \n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    model, tokenizer = load_model_and_tokenizer(model_id, peft=peft)\n",
    "    if gradient_checkpointing == True or peft == 'qlora':\n",
    "        model.config.use_cache = False\n",
    "\n",
    "    dataset = make_dummy_dataset()\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        gradient_checkpointing=gradient_checkpointing,\n",
    "        output_dir='./result',\n",
    "        num_train_epochs=1\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        train_model(model, dataset, training_args)\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(e)\n",
    "        else:\n",
    "            raise(e)\n",
    "    finally:\n",
    "        del model, dataset\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfd2202-c440-41ac-9179-a09b4ac69b51",
   "metadata": {},
   "source": [
    "**배치 크기를 변경하며 메모리 사용량 측정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bc01980-cef1-41e9-975c-2043621d2a89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory usage: 0.000 GB\n",
      "Batch size: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "250cba49ce3c4af2a9c7b4b79386f5c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory usage: 2.599 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory usage: 10.586 GB\n",
      "Optimizer state memory usage: 4.961 GB\n",
      "Gradient memory usage: 2.481 GB\n",
      "GPU memory usage: 0.016 GB\n",
      "Batch size: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d4a72e40fce4fa7bec05c945e89b1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory usage: 2.615 GB\n",
      "GPU memory usage: 11.113 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer state memory usage: 4.961 GB\n",
      "Gradient memory usage: 2.481 GB\n",
      "GPU memory usage: 0.016 GB\n",
      "Batch size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c825869a51241d28a9466d333484b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory usage: 2.615 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory usage: 12.164 GB\n",
      "Optimizer state memory usage: 4.961 GB\n",
      "Gradient memory usage: 2.481 GB\n",
      "GPU memory usage: 0.016 GB\n"
     ]
    }
   ],
   "source": [
    "cleanup()\n",
    "print_gpu_utilization()\n",
    "\n",
    "for batch_size in [4, 8, 16]:\n",
    "    gpu_memory_experiment(batch_size)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19eff37-f577-4c39-b666-3eb6ccacf95a",
   "metadata": {},
   "source": [
    "# 단일 GPU 효율적으로 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b99a52-e318-4248-9852-073d0c1aecd5",
   "metadata": {},
   "source": [
    "## 그레이디언트 누적"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0d69eb-0ce7-43d3-a408-b58e18275ac7",
   "metadata": {},
   "source": [
    "**그레이디언트 누적 관련 부분** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81443b3b-ee33-4ccb-b753-1c34c8c707a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, training_args):\n",
    "    if training_args.gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    train_dataloader = DataLoader(dataset, batch_size=training_args.per_device_train_batch_size)\n",
    "    optimizer = AdamW(model.parameters())\n",
    "    model.train()\n",
    "    gpu_utilization_printed = False\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader, start=1):\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss = loss / training_args.gradient_accumulation_steps # 4일 경우, 손실을 4로 나눠서 역전파를 수행\n",
    "        loss.backward()\n",
    "\n",
    "        if step % training_args.gradient_accumulation_steps == 0: # 배치 크기가 4배로 커진 것과 동일한 효과\n",
    "            optimizer.step()\n",
    "            gradients_memory = estimate_memory_of_gradients(model)\n",
    "            optimizer_memory = estimate_memory_of_optimizer(optimizer)\n",
    "            if not gpu_utilization_printed:\n",
    "                print_gpu_utilization()\n",
    "                gpu_utilization_printed = True\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Optimizer state memory usage: {optimizer_memory / (1024 ** 3):.3f} GB\")\n",
    "    print(f\"Gradient memory usage: {gradients_memory / (1024 ** 3):.3f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63b8459-8162-4aa1-8005-bd35ab9db1fb",
   "metadata": {},
   "source": [
    "**그레이디언트 누적을 적용했을 때의 메모리 사용량**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16479507-aaa1-47a3-a92e-fe69a1ffaebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory usage: 8.104 GB\n",
      "Batch size: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8562776f0c4c4c47a47058002eb0988b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory usage: 10.703 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory usage: 18.674 GB\n",
      "Optimizer state memory usage: 4.961 GB\n",
      "Gradient memory usage: 2.481 GB\n",
      "GPU memory usage: 8.104 GB\n"
     ]
    }
   ],
   "source": [
    "cleanup()\n",
    "print_gpu_utilization()\n",
    "\n",
    "gpu_memory_experiment(batch_size=4, gradient_accumulation_steps=4)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bf256a-998b-4482-babd-9ec650a433af",
   "metadata": {},
   "source": [
    "## 그레이디언트 체크포인팅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1fa005-5720-4b0d-a05b-75977fd40395",
   "metadata": {},
   "source": [
    "**그레이디언트 체크포인팅 사용 시 메모리 사용량**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e754e82-b75b-4919-a373-4230a818e887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory usage: 0.000 GB\n"
     ]
    }
   ],
   "source": [
    "cleanup()\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88f7eeb6-d96e-4a28-80ff-29d9317b2e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe74fdb43d548b09f521fea7e474e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory usage: 5.198 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory usage: 12.888 GB\n",
      "Optimizer state memory usage: 4.961 GB\n",
      "Gradient memory usage: 2.481 GB\n",
      "GPU memory usage: 2.615 GB\n"
     ]
    }
   ],
   "source": [
    "gpu_memory_experiment(batch_size=16, gradient_checkpointing=True)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb732408-adb4-4b08-acb1-008e339f9cec",
   "metadata": {},
   "source": [
    "# 효율적인 학습 방법(PEFT): LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60654eb2-bd59-47ff-af38-47bebbbb9853",
   "metadata": {},
   "source": [
    "## LoRA 학습 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef2ed2e-9009-465f-bbcf-f912409a3bb4",
   "metadata": {},
   "source": [
    "**모델을 불러오면서 LoRA 적용하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2dfffae-50d6-4396-ae3c-280ff6ccd727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "def load_model_and_tokenizer(model_id, peft=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    if peft is None:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map={\"\": 0})\n",
    "    elif peft == 'lora':\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map={\"\": 0})\n",
    "        lora_config = LoraConfig(\n",
    "                        r=8,\n",
    "                        lora_alpha=32,\n",
    "                        target_modules=[\"query_key_value\"],\n",
    "                        lora_dropout=0.05,\n",
    "                        bias=\"none\",\n",
    "                        task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "    print_gpu_utilization()\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ff016cf-6bc3-4c9e-b1df-5de0a945359b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory usage: 0.000 GB\n",
      "Batch size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f7601f99f14fadb7a490234005772e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,572,864 || all params: 1,333,383,168 || trainable%: 0.11796039111242178\n",
      "GPU memory usage: 2.602 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory usage: 4.732 GB\n",
      "Optimizer state memory usage: 0.006 GB\n",
      "Gradient memory usage: 0.003 GB\n",
      "GPU memory usage: 0.016 GB\n"
     ]
    }
   ],
   "source": [
    "cleanup()\n",
    "print_gpu_utilization()\n",
    "\n",
    "gpu_memory_experiment(batch_size=16, peft='lora')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87899a2-987f-40ed-8683-1561164ab3f8",
   "metadata": {},
   "source": [
    "**4비트 양자화 모델 불러오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34ec3731-bec9-4e68-a902-c4f92166fa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d357aac8-6825-4e41-a210-be062fbc628b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85bfe28b1ca41cc9418de7efb1a512f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2dc35af9-c05e-43cd-9464-96cf4bb5a43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(30080, 2048)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear4bit(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=2048, out_features=30080, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6712e6-60cc-45ac-aa13-6c34494759c1",
   "metadata": {},
   "source": [
    "**QLoRA 모델 불러오기 추가**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6107294-1f09-4b00-92cc-67d56049f4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 00:32:29.189149: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-15 00:32:29.283063: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-15 00:32:29.287242: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-11-15 00:32:29.287251: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-11-15 00:32:29.311409: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-15 00:32:29.759876: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-11-15 00:32:29.759923: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-11-15 00:32:29.759929: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1521bc37-1819-4409-977d-86eedcc64977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_id, peft=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if peft is None:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map={\"\":0})\n",
    "\n",
    "    elif peft == 'lora':\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map={\"\":0})\n",
    "        lora_config = LoraConfig(\n",
    "                        r=8,\n",
    "                        lora_alpha=32,\n",
    "                        target_modules=[\"query_key_value\"],\n",
    "                        lora_dropout=0.05,\n",
    "                        bias=\"none\",\n",
    "                        task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "    elif peft == 'qlora':\n",
    "        lora_config = LoraConfig(\n",
    "                        r=8,\n",
    "                        lora_alpha=32,\n",
    "                        target_modules=[\"query_key_value\"],\n",
    "                        lora_dropout=0.05,\n",
    "                        bias=\"none\",\n",
    "                        task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
    "        model.gradient_checkpointing_enable()\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "            \n",
    "    \n",
    "    print_gpu_utilization()\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36c3220-bfe9-4c72-a279-cd7bc762ecdc",
   "metadata": {},
   "source": [
    "**QLoRA를 적용했을 때의 GPU 메모리 사용량 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63bf8c9b-8e88-4710-9dd5-7622eb84b93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory usage: 0.000 GB\n",
      "Batch size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ae5357cdfe4a8da59649900364d08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,572,864 || all params: 1,333,383,168 || trainable%: 0.11796039111242178\n",
      "GPU memory usage: 1.167 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory usage: 6.972 GB\n",
      "Optimizer state memory usage: 0.012 GB\n",
      "Gradient memory usage: 0.006 GB\n",
      "GPU memory usage: 0.016 GB\n"
     ]
    }
   ],
   "source": [
    "cleanup()\n",
    "print_gpu_utilization()\n",
    "\n",
    "gpu_memory_experiment(batch_size=16, peft='qlora')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746d4ede-fca9-4a03-ac67-88beb5395dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
