# 사전 학습과 지도 미세 조정

## LLM의 사전 학습
LLM은 보통 인터넷상에 있는 다양한 텍스트 데이터를 수집한 대용량의 텍스트로 사전 학습한다.
- LLM은 딥러닝 기반의 언어 모델이며, 다음 단어를 예측하는 언어 모델링을 통해 텍스트를 이해하는 방법을 학습한다.
- 사전 학습 동안은 LLM이 언어에 대한 전체적인 이해도가 높아지고 바로 다음에 올 단어를 점점 더 잘 예측하게 된다.

## 지도 미세 조정
LLM이 사용자의 요청에 적절히 응답하기 위해서는 요청의 형식을 적절히 해석하고, 응답의 형태를 적절히 작성하여, 요청과 응답이 잘 연결되도록 추가로 학습해야 하며, 이를 지도 미세 조정(supervised fine-tuning)이라고 한다.
- 지도 미세 조정을 통해 LLM은 사용자의 요청에 맞춰 응답하도록 학습하게 되는데 이를 정렬(aligment)이라고 한다.
- 지도 미세 조정에 사용하는 데이터 셋을 지시 데이터셋(instruction dataset)이라고 한다.

**지시 데이터셋의 형태는?**
- 지시사항은 사용자의 요구사항을 표현한 문장이다.
- 입력에는 답변을 하는데 필요한 데이터가 들어간다.
- 출력은 지시사항과 입력을 바탕으로 한 정답 응답이다.
- 텍스트는 지시사항, 입력, 출력을 정해진 포맷으로 하나로 묶은 데이터다.

## 좋은 지시 데이터셋이 갖춰야 할 조건
지시 데이터셋을 준비한다고 했을 때 데이터의 양, 품질, 질문의 형식, 답변의 형식 등 다양한 측면에서 고민해 봐야 한다.
- 메타는 모델을 정렬하는 데 선별한 1,000개 정도의 지시 데이터셋으로도 가능하다고 한다.
- 지시 데이터셋에서 지시사항이 다양한 형태로 되어 있고 응답 데이터의 품질이 높을수록 정렬한 모델의 답변 품질이 높아진다고 한다.
    - 메타에서는 이런 결과를 바탕으로 '피상적인 정렬 가설(superficial alignment hypothesis)'을 주장했다.
    - 피상적인 정렬 가설이란, 모델의 지식이나 능력은 사전 학습 단계에서 대부분 학습하고 정렬 데이터를 통해서는 답변의 형식이나 모델이 능력과 지식을 어떻게 나열할지 정도만 추가로 배우기 때문에 적은 정렬 데이터로도 사용자가 원하는 형태의 답변을 생성할 수 있다는 가설이다.
- 마이크로소프트는 지시 데이터셋의 품질을 높이면 더 작은 데이터셋과 더 작은 모델로도 높은 성능을 달성할 수 있다고 주장한다.

</br>

# 선호 데이터셋, 강화 학습

## 선호 데이터셋을 사용한 모델 만들기

**선호 데이터셋**  
선호 데이터(chosen data), 비선호 데이터(rejected data)의 형태로 두 데이터 중 사람이 더 선호하는 데이터를 선택한 데이터셋을 말한다.
- 예를 들어, 코드 가독성 판단을 위한 데이터를 만든다고 가정할 때, 코드 자체에 대한 점수를 부여하는 것은 어렵지만, 두 코드를 비교해 가독성이 높은 코드를 선택하는 것은 비교적 쉽기 때문에 선호 데이터셋을 구축해 모델을 만든다.

## 강화 학습
강화 학습에서는 에이전트가 환경에서 행동을 한다.
- 행동에 따라 환경의 상태가 바뀌고 행동에 대한 보상이 생긴다.
- 에이전트는 이 변환된 상태를 인식하고 보상을 받는다.
- 에이전트는 가능하면 더 많은 보상을 받을 수 있도록 행동을 수정하면서 학습한다.
- 에이전트가 연속적으로 수행하는 행동의 모음 = 에피소드

## PPO: 보상 해킹 피하기
평가 모델의 높은 점수를 받는 과정에서 다른 능력이 감소하거나 평가 점수만 높게 받을 수 있는 우회로를 찾는 현상을 보상 해킹이라고 한다.

**근접 정책 최적화(Proximal Preference Optimization)**  
지도 미세 조정 모델을 기준으로 학습하는 모델이 너무 멀지 않게 가까운 범위에서 리워드 모델의 높은 점수를 찾도록 하는 방법

## RLHF: 멋지지만 피할 수 있다면...
모델을 학습시킬 때 참고 모델, 학습 모델, 리워드 모델 총 3개의 모델이 필요하기 때문에 GPU와 같은 리소스가 더 많이 필요하다. 또한, 강화 학습 자체가 하이퍼파라미터에 민감하고 학습이 불안정하다.

</br>

# 강화 학습이 꼭 필요할까?

## 기각 샘플링(Rejection Sampling)
지도 미세 조정을 마친 LLM을 통해 여러 응답을 생성하고 그중에서 리워드 모델이 가장 높은 점수를 준 응답을 모아 다시 지도 미세 조정을 수행하는 방법
- 강화 학습을 사용하지 않기 때문에 학습이 비교적 안정적이고 간단하고 직관적인 방법임에도 효과가 좋아 많이 활용된다.

## DPO: 선호 데이터셋을 직접 학습하기
좋은 리워드 모델을 만들고 관리하는 것도 쉽지 않은 일이고 리워드 모델을 만든다고 하더라도 리워드 모델에 강화 학습으로 사람의 선호를 반영한다는 것도 쉽지 않다.

RLHF는 선호 데이터셋으로 리워드 모델을 만들고 언어 모델의 출력을 평가하면서 강화 학습을 진행한다.  
DPO에서는 선호 데이터셋을 직접 언어 모델에 학습시킨다.